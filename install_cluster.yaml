- name: Setup FlashBlade and Install Kafka/Elasticsearch
  hosts: localhost
  collections:
  - purestorage.flashblade
  vars:
    namespace: default
    cp_version: 6.1.0
    es_version: 7.11.0
    s3_accountname: default
    s3_username: ke-user
    kafka_bucket: kafka-tiered-storage
    zk_storageclass: pure-block
    kafka_storageclass: pure-file
    elastic_name: ke-elastic
    elastic_storageclass: pure-file
  vars_files:
    # FlashBlade mgmt inforation (with token) stored in a separate file (not in version control).
    # This file should include variables: FB_MGMTVIP, FB_DATAVIP, and FB_TOKEN
    - flashblade_creds.yaml
  tasks:
  - name: Check kubectl finds storageclass
    shell: kubectl get storageclass pure-file

  - name: Ensure python modules installed
    pip:
      name: purity_fb

  - name: Create service accounts
    purefb_s3acc:
      name: "{{ s3_accountname }}"
      fb_url: "{{ FB_MGMTVIP }}"
      api_token: "{{ FB_TOKEN }}"
      state: present

  - name: Check if S3 key secret already exists
    shell: "kubectl -n={{ namespace }} get secret ke-s3-keys"
    ignore_errors: yes
    register: keysecret_exists

  - name: Create user within account
    when: keysecret_exists is failed
    purefb_s3user:
      account: "{{ s3_accountname }}"
      name: "{{ s3_username }}"
      access_key: true
      fb_url: "{{ FB_MGMTVIP }}"
      api_token: "{{ FB_TOKEN }}"
      state: present
    register: result
  - set_fact:
      keypair: "{{ result['s3user_info']['fb_s3user'] }}"
    when: keysecret_exists is failed

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      create: yes
      line: "PUREFB_URL={{ FB_MGMTVIP }} PUREFB_API={{ FB_TOKEN }} ansible localhost -m purefb_s3user -a \"account={{ s3_accountname }} name={{ s3_username }} state=absent\""

  - name: Create k8s secret
    when: keysecret_exists is failed
    shell: "kubectl -n={{ namespace }} create secret generic ke-s3-keys --from-literal=access-key='{{ keypair['access_id'] }}' --from-literal=secret-key='{{ keypair['access_key'] }}' --dry-run=client -o yaml | kubectl apply -f -"
  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete secret ke-s3-keys"

  - name: Create Buckets for Kafka and Elasticsearch
    purefb_bucket:
      name: "{{ item }}"
      account: "{{ s3_accountname }}"
      fb_url: "{{ FB_MGMTVIP }}"
      api_token: "{{ FB_TOKEN }}"
      state: present
    loop:
      - "{{ kafka_bucket }}"
      - "es-snaprepo-{{ elastic_name }}"
      - "warehouse"

  - name: Create Zookeeper Headless Service
    shell: |
      cat <<EOF | kubectl apply -f -
      apiVersion: v1
      kind: Service
      metadata:
        name: kafka-zk-hs
        namespace: "{{ namespace }}"
        labels:
          app: kafka-zk
      spec:
        ports:
        - port: 2888
          name: server
        - port: 3888
          name: leader-election
        clusterIP: None
        selector:
          app: kafka-zk

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete service kafka-zk-hs"

  - name: Create Zookeeper Service
    shell: |
      cat <<EOF | kubectl apply -f -
      apiVersion: v1
      kind: Service
      metadata:
        name: kafka-zk-cs
        namespace: "{{ namespace }}"
        labels:
          app: kafka-zk
      spec:
        ports:
        - port: 2181
          name: client
        selector:
          app: kafka-zk
  
  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete service kafka-zk-cs"

  - name: Create Zookeeper PodDisruptionBudget
    shell: |
      cat <<EOF | kubectl apply -f -
      apiVersion: policy/v1beta1
      kind: PodDisruptionBudget
      metadata:
        name: kafka-zk-pdb
        namespace: "{{ namespace }}"
      spec:
        selector:
          matchLabels:
            app: kafka-zk
        maxUnavailable: 1

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete pdb kafka-zk-pdb"

  - name: Create Zookeeper Statefulset
    shell: |
      cat <<EOF | kubectl apply -f -
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: kafka-zk
          namespace: "{{ namespace }}"
        spec:
          selector:
            matchLabels:
              app: kafka-zk
          serviceName: kafka-zk-hs
          replicas: 3
          updateStrategy:
            type: RollingUpdate
          podManagementPolicy: OrderedReady
          template:
            metadata:
              labels:
                app: kafka-zk
            spec:
              affinity:
                podAntiAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    - labelSelector:
                        matchExpressions:
                          - key: "app"
                            operator: In
                            values:
                            - kafka-zk
                      topologyKey: "kubernetes.io/hostname"
              containers:
              - name: kubernetes-zookeeper
                imagePullPolicy: Always
                image: "k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10"
                resources:
                  requests:
                    memory: "1Gi"
                    cpu: "0.5"
                ports:
                - containerPort: 2181
                  name: client
                - containerPort: 2888
                  name: server
                - containerPort: 3888
                  name: leader-election
                command:
                - sh
                - -c
                - "start-zookeeper \
                  --servers=3 \
                  --data_dir=/var/lib/zookeeper/data \
                  --data_log_dir=/var/lib/zookeeper/data/log \
                  --conf_dir=/opt/zookeeper/conf \
                  --client_port=2181 \
                  --election_port=3888 \
                  --server_port=2888 \
                  --tick_time=2000 \
                  --init_limit=10 \
                  --sync_limit=5 \
                  --heap=512M \
                  --max_client_cnxns=60 \
                  --snap_retain_count=3 \
                  --purge_interval=12 \
                  --max_session_timeout=40000 \
                  --min_session_timeout=4000 \
                  --log_level=INFO"
                readinessProbe:
                  exec:
                    command:
                    - sh
                    - -c
                    - "zookeeper-ready 2181"
                  initialDelaySeconds: 10
                  timeoutSeconds: 5
                livenessProbe:
                  exec:
                    command:
                    - sh
                    - -c
                    - "zookeeper-ready 2181"
                  initialDelaySeconds: 10
                  timeoutSeconds: 5
                volumeMounts:
                - name: datadir
                  mountPath: /var/lib/zookeeper
              securityContext:
                runAsUser: 1000
                fsGroup: 1000
          volumeClaimTemplates:
          - metadata:
              name: datadir
            spec:
              accessModes: [ "ReadWriteOnce" ]
              storageClassName: "{{ zk_storageclass }}"
              resources:
                requests:
                  storage: 10Gi

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete statefulset kafka-zk"

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete pvc -l app=kafka-zk &"

  - name: Check if JXM jar configmap already exists
    shell: "kubectl -n={{ namespace }} get configmap jmx-prometheus-javaagent-jar"
    ignore_errors: yes
    register: jmxjar_exists
  - name: Install JMX jar as a binary configmap
    when: jmxjar_exists is failed
    shell: "kubectl -n={{ namespace }} create configmap --from-file=jmx_prometheus_javaagent-0.13.0.jar jmx-prometheus-javaagent-jar"

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete configmap jmx-prometheus-javaagent-jar"

  - name: Create JMX Prometheus Config
    shell: |
      cat <<EOF | kubectl apply -f -
      kind: ConfigMap 
      apiVersion: v1 
      metadata:
        name: kafka-jmx-prom-config
        namespace: "{{ namespace }}"
      data:
        kafka-2_0_0.yml: |
          lowercaseOutputLabelNames: false
          lowercaseOutputName: true

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete configmap kafka-jmx-prom-config"

  - name: Create Confluent Kafka ENV Config
    shell: |
      cat <<EOF | kubectl apply -f -
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: confluent-env-config
        namespace: "{{ namespace }}"
      data:
        KAFKA_ZOOKEEPER_CONNECT: kafka-zk-cs:2181
        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://:9092
        KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
        KAFKA_CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: confluentkafka:9092
        KAFKA_COMPRESSION_TYPE: producer
        KAFKA_LOG_SEGMENT_BYTES: "104857600"
        KAFKA_LOG_FLUSH_INTERVAL_MS: "1000"
        KAFKA_LOG_FLUSH_INTERVAL_MESSAGES: "1000"
        KAFKA_NUM_PARTITIONS: "12"
        KAFKA_DEFAULT_REPLICATION_FACTOR: "2"
        KAFKA_CONFLUENT_TIER_ENABLE: "true"
        KAFKA_CONFLUENT_TIER_FEATURE: "true"
        KAFKA_CONFLUENT_TIER_BACKEND: S3
        KAFKA_CONFLUENT_TIER_S3_BUCKET: "{{ kafka_bucket }}"
        KAFKA_CONFLUENT_TIER_S3_REGION: "us-west-2"
        KAFKA_CONFLUENT_TIER_S3_AWS_ENDPOINT_OVERRIDE: "http://{{ FB_DATAVIP }}"
        KAFKA_CONFLUENT_TIER_LOCAL_HOTSET_MS: "0"
        KAFKA_CONFLUENT_TIER_ARCHIVER_NUM_THREADS: "8"
        KAFKA_CONFLUENT_TIER_FETCHER_NUM_THREADS: "16"
        KAFKA_CONFLUENT_TIER_TOPIC_DELETE_CHECK_INTERVAL_MS: "30000"
        KAFKA_CONFLUENT_TIER_METADATA_REPLICATION_FACTOR: "3"
        KAFKA_CONFLUENT_BALANCER_ENABLE: "true"
        KAFKA_CONFLUENT_BALANCER_THROTTLE_BYTES_PER_SECOND: "10737418240"
        KAFKA_JMX_PORT: "8091"
        KAFKA_OPTS: "-javaagent:/opt/kafka/prometheus/jmx_prometheus_javaagent-0.13.0.jar=7071:/opt/kafka/prometheus/kafka-2_0_0.yml"

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete configmap confluent-env-config"

  - name: Wait for Kafka-ZK to be ready
    shell: "kubectl -n={{ namespace }} rollout status statefulset.apps/kafka-zk"

  - name: Create Service for Kafka
    shell: |
      cat <<EOF | kubectl apply -f -
      apiVersion: v1
      kind: Service
      metadata:
        name: confluentkafka
        namespace: "{{ namespace }}"
        labels:
          app: confluentkafka
      spec:
        clusterIP: None
        ports:
        - name: kafka-port
          port: 9092
        - name: mtx-port
          port: 7071
        selector:
          app: confluentkafka

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete service confluentkafka"

  - name: Create Statefulset for Kafka
    shell: |
      cat <<EOF | kubectl apply -f -
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: confluentkafka
        namespace: "{{ namespace }}"
      spec:
        serviceName: confluentkafka
        replicas: 4
        selector:
          matchLabels:
            app: confluentkafka
        template:
          metadata:
            labels:
              app: confluentkafka
          spec:
            securityContext:
              fsGroup: 1000
            containers:
            - name: cp-server
              image: "confluentinc/cp-server:{{ cp_version }}"
              envFrom:
              - configMapRef:
                  name: confluent-env-config
              env:
              - name: AWS_ACCESS_KEY_ID
                valueFrom:
                  secretKeyRef:
                    name: ke-s3-keys
                    key: access-key
              - name: AWS_SECRET_ACCESS_KEY
                valueFrom:
                  secretKeyRef:
                    name: ke-s3-keys
                    key: secret-key
              ports:
              - containerPort: 9092
                name: kafka-port
              - containerPort: 7071
                name: mtx-port
              imagePullPolicy: Always
              volumeMounts:
              - name: broker-data
                mountPath: /var/lib/kafka/data
              - name: kafka-jmx-prom-config-vol
                mountPath: /opt/kafka/prometheus/kafka-2_0_0.yml
                subPath: kafka-2_0_0.yml
              - name: jmx-prometheus-javaagent-jar-vol
                mountPath: /opt/kafka/prometheus/jmx_prometheus_javaagent-0.13.0.jar
                subPath: jmx_prometheus_javaagent-0.13.0.jar
              resources:
                requests:
                  memory: 32Gi
                  cpu: 8
                limits:
                  memory: 64Gi
                  cpu: 12
            volumes:
            - name: kafka-jmx-prom-config-vol
              configMap:
                name: kafka-jmx-prom-config
            - name: jmx-prometheus-javaagent-jar-vol
              configMap:
                name: jmx-prometheus-javaagent-jar
        volumeClaimTemplates:
        - metadata:
            name: broker-data
          spec:
            storageClassName: "{{ kafka_storageclass }}"
            accessModes:
            - ReadWriteOnce
            resources:
              requests:
                storage: 1Ti

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete statefulset confluentkafka"

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete pvc -l app=confluentkafka &"

  - name: Wait for Kafka to be ready
    shell: "kubectl -n={{ namespace }} rollout status statefulset.apps/confluentkafka"

  - name: Create service for confluent-control-center
    shell: |
      cat <<EOF | kubectl apply -f -
      apiVersion: v1
      kind: Service
      metadata:
        name: confluent-control-center
        namespace: "{{ namespace }}"
      spec:
        type: NodePort
        ports:
        - port: 9021
          nodePort: 30921
          targetPort: 9021
        selector:
          app: confluent-control-center

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete service confluent-control-center"
      
  - name: Create PVC for confluent-control-center
    shell: |
      cat <<EOF | kubectl apply -f -
      kind: PersistentVolumeClaim
      apiVersion: v1
      metadata:
        name: ccc-data-claim
        namespace: "{{ namespace }}"
        labels:
          app: confluent-control-center
      spec:
        storageClassName: pure-file
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 100Gi

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete pvc ccc-data-claim &"

  - name: Create deployment for confluent-control-center
    shell: |
      cat <<EOF | kubectl apply -f -
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: confluent-control-center
        namespace: "{{ namespace }}"
      spec:
        selector:
          matchLabels:
            app: confluent-control-center
        strategy:
          type: Recreate
        template:
          metadata:
            labels:
              app: confluent-control-center
          spec:
            containers:
            - name: ccc
              image: "confluentinc/cp-enterprise-control-center:{{ cp_version }}"
              env:
              - name: CONTROL_CENTER_ZOOKEEPER_CONNECT
                value: "kafka-zk-cs:2181"
              - name: CONTROL_CENTER_BOOTSTRAP_SERVERS
                value: "confluentkafka:9092"
              - name: CONTROL_CENTER_KSQL_ENABLE
                value: "false"
              - name: CONTROL_CENTER_REPLICATION_FACTOR
                value: "2"
              - name: CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS
                value: "1"
              - name: CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS
                value: "1"
              - name: CONFLUENT_METRICS_TOPIC_REPLICATION
                value: "2"
              ports:
              - containerPort: 9021
              resources:
                requests:
                  memory: "8G"
                  cpu: 2
              volumeMounts:
              - name: ccc-data
                mountPath: /var/lib/confluent-control-center
              imagePullPolicy: Always
            volumes:
            - name: ccc-data
              persistentVolumeClaim:
                claimName: ccc-data-claim

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete deployment confluent-control-center"

  - name: Wait for confluent-control-center to be ready
    shell: "kubectl -n={{ namespace }} rollout status deployment.apps/confluent-control-center"

  - name: Create PVC for Elasticsearch Filesystem Snapshot Repository
    shell: |
      cat <<EOF | kubectl apply -f -
      kind: PersistentVolumeClaim
      apiVersion: v1
      metadata:
        name: "{{ elastic_name }}-snap-repo-claim"
        namespace: "{{ namespace }}"
      spec:
        storageClassName: pure-file
        accessModes:
        - ReadWriteMany
        resources:
          requests:
            storage: 10Ti

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete pvc {{ elastic_name }}-snap-repo-claim &"

  - name: Create ECK cluster
    shell: |
      cat <<EOF | kubectl apply -f -
      apiVersion: elasticsearch.k8s.elastic.co/v1
      kind: Elasticsearch
      metadata:
        name: "{{ elastic_name }}"
        namespace: "{{ namespace }}"
      spec:
        version: "{{ es_version }}"
        nodeSets:
        - name: all-nodes
          count: 5
          podTemplate:
            spec:
              containers:
              - name: elasticsearch
                volumeMounts:
                - name: es-snap-repo-vol
                  mountPath: /es-snaps
                resources:
                  limits:
                    memory: 24Gi
                    cpu: 12
              initContainers:
              - name: install-plugins
                command:
                - sh
                - -c
                - |
                  bin/elasticsearch-plugin remove repository-s3
                  bin/elasticsearch-plugin install --batch repository-s3
              - name: add-access-keys
                env:
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      name: ke-s3-keys
                      key: access-key
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      name: ke-s3-keys
                      key: secret-key
                command:
                  - sh
                  - -c
                  - |
                    env | grep AWS_ACCESS_KEY_ID | awk -F= '{print \$2}' | bin/elasticsearch-keystore add --stdin --force s3.client.default.access_key
                    env | grep AWS_SECRET_ACCESS_KEY | awk -F= '{print \$2}' | bin/elasticsearch-keystore add --stdin --force s3.client.default.secret_key
              volumes:
              - name: es-snap-repo-vol
                persistentVolumeClaim:
                  claimName: "{{ elastic_name }}-snap-repo-claim"
          config:
            node.store.allow_mmap: false
            node.roles: ["master", "data_hot", "ingest", "ml", "data_warm", "data_cold", "data_content"]
            thread_pool.snapshot.max: 8
            path.repo: ["/es-snaps"]
            indices.recovery.max_bytes_per_sec: "200mb"
          volumeClaimTemplates:
          - metadata:
              name: elasticsearch-data
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 8Ti
              storageClassName: "{{ elastic_storageclass }}"

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete elasticsearch {{ elastic_name }}"

  - name: Create Kibana instance
    shell: |
      cat <<EOF | kubectl apply -f -
      apiVersion: kibana.k8s.elastic.co/v1
      kind: Kibana
      metadata:
        name: "{{ elastic_name }}"
        namespace: "{{ namespace }}"
      spec:
        version: "{{ es_version }}"
        count: 1
        elasticsearchRef:
          name: "{{ elastic_name }}"
        http:
          service:
            spec:
              type: NodePort
              ports:
              - name: https
                nodePort: 30561
                port: 5601
                targetPort: 5601

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete kibana {{ elastic_name }}"

  - name: Create client pod for commands
    shell: "kubectl -n={{ namespace }} run temp-jump --image=centos:7 --restart=Never -- tail -f /dev/null"
    ignore_errors: yes

  - name: Wait for elasticsearch cluster ready
    command: "kubectl -n={{ namespace }} get elasticsearch {{ elastic_name }} -o jsonpath='{.status.health}'"
    register: cmd_res
    retries: 60
    delay: 5
    until: ("green" in cmd_res.stdout)

  - name: Get ES password
    shell: "kubectl -n={{ namespace }} get secret {{ elastic_name }}-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode"
    register: es_password

  - name: Set ES password fact
    set_fact:
      es_password: "{{ es_password.stdout }}"
  - set_fact:
      es_endpoint: "https://{{ elastic_name }}-es-http:9200"

  - name: Create beats configmap
    shell: |
      cat <<EOF | kubectl apply -f -
      kind: ConfigMap 
      apiVersion: v1 
      metadata:
        name: kafka-beats-config
        namespace: "{{ namespace }}"
      data:
        filebeat.yml: |
          filebeat.inputs:
          - type: kafka
            hosts:
              - confluentkafka:9092
            topics: ["flogs", "unmatched"]
            group_id: "kafka-beats"

          setup.template.settings:
            index.number_of_shards: 12
            index.number_of_replicas: 1
            index.refresh_interval: 30s

          output.elasticsearch:
            hosts: "\${ELASTICSEARCH_HOST}:9200"
            protocol: 'https'
            username: elastic
            password: "\${ELASTICSEARCH_PASSWORD}"
            ssl.enabled: true
            ssl.verification_mode: "none"
            worker: 4
            bulk_max_size: 4096

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete configmap kafka-beats-config"

  - name: Create beats deployment
    shell: |
      cat <<EOF | kubectl apply -f -
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: kafka-beats
        namespace: "{{ namespace }}"
      spec:
        replicas: 6
        selector:
          matchLabels:
            app: kafka-beats
        template:
          metadata:
            labels:
              app: kafka-beats
          spec:
            containers:
            - name: beats
              image: "docker.elastic.co/beats/filebeat:{{ es_version }}"
              env:
              - name: ELASTICSEARCH_HOST
                value: "{{ elastic_name }}-es-http"
              - name: ELASTICSEARCH_PASSWORD
                valueFrom:
                  secretKeyRef:
                    name: "{{ elastic_name }}-es-elastic-user"
                    key: elastic
              volumeMounts:
              - name: beats-config-vol
                mountPath: /usr/share/filebeat/filebeat.yml
                subPath: filebeat.yml
            volumes:
            - name: beats-config-vol
              configMap:
                name: kafka-beats-config

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete deployment kafka-beats"

  - name: Create S3 Snapshot Repository
    shell: |
      kubectl exec -i "-n={{ namespace }}" temp-jump -- curl -sS -u "elastic:{{ es_password }}" -k -X PUT "{{ es_endpoint }}/_snapshot/fb_s3_repository?pretty" -H 'Content-Type: application/json' -d '
        {
          "type": "s3",
          "settings": {
            "bucket": "es-snaprepo-{{ elastic_name }}",
            "endpoint": "{{ FB_DATAVIP }}",
            "protocol": "http",
            "max_restore_bytes_per_sec": "1gb",
            "max_snapshot_bytes_per_sec": "500mb"
          }
        }'

  - name: Create NFS Snapshot Repository
    shell: |
      kubectl exec -it "-n={{ namespace }}" temp-jump -- curl -sS -u "elastic:{{ es_password }}" -k -X PUT "{{ es_endpoint }}/_snapshot/fb_nfs_repository?pretty" -H 'Content-Type: application/json' -d '
        {
          "type": "fs",
          "settings": {
            "location": "/es-snaps",
            "max_restore_bytes_per_sec": "1gb",
            "max_snapshot_bytes_per_sec": "500mb"
          }
        }'

  - name: Check snapshot repositories
    shell: "kubectl -n={{ namespace }} exec -it temp-jump -- curl -sS -u elastic:{{ es_password }} -k {{ es_endpoint }}/_snapshot?pretty"
    register: curl_out
  - debug: var=curl_out.stdout_lines

  - name: Create S3 Snapshot Schedule
    when: 0 > 1
    shell: |
      kubectl "-n={{ namespace }}" exec -it temp-jump -- curl -sS -u "elastic:{{ es_password }}" -k -X PUT "{{ es_endpoint }}/_slm/policy/hourly-snapshots?pretty" -H 'Content-Type: application/json' -d'
        {
          "schedule": "0 30 * * * ?",
          "name": "<hourly-snap-{now/d}>",
          "repository": "fb_s3_repository",
          "config": {
            "indices": ["filebeat-*"],
            "ignore_unavailable": true,
            "include_global_state": false
          },
          "retention": {
            "expire_after": "30d",
            "min_count": 24,
            "max_count": 720
          }
        }'

  - name: Create NFS Snapshot Schedule
    shell: |
      kubectl "-n={{ namespace }}" exec -it temp-jump -- curl -sS -u "elastic:{{ es_password }}" -k -X PUT "{{ es_endpoint }}/_slm/policy/hourly-snapshots-nfs?pretty" -H 'Content-Type: application/json' -d'
        {
          "schedule": "0 00 * * * ?",
          "name": "<hourly-snap-{now/d}>",
          "repository": "fb_nfs_repository",
          "config": {
            "indices": ["filebeat-*"],
            "ignore_unavailable": true,
            "include_global_state": false
          },
          "retention": {
            "expire_after": "10d",
            "min_count": 24,
            "max_count": 48
          }
        }'

  - name: Create ILM Policy for filebeats
    shell: |
      kubectl "-n={{ namespace }}" exec -it temp-jump -- curl -sS -u "elastic:{{ es_password }}" -k -X PUT "{{ es_endpoint }}/_ilm/policy/filebeat?pretty" -H 'Content-Type: application/json' -d'
        { 
          "policy": {
            "phases" : { 
              "hot" : { 
                "actions" : { 
                  "rollover" : { 
                    "max_age" : "12h", 
                    "max_size" : "500gb" 
                  } 
                } 
              }, 
              "warm" : { 
                "min_age" : "24h", 
                "actions" : { 
                  "forcemerge" : { 
                    "max_num_segments" : 1 
                  } 
                } 
              }, 
              "cold" : { 
                "min_age" : "30h", 
                "actions" : {
                  "searchable_snapshot": {
                    "snapshot_repository" : "fb_s3_repository"
                  }
                } 
              }, 
              "delete" : { 
                "min_age" : "10d", 
                "actions" : { 
                  "delete" : { } 
                } 
              } 
            }
          }
        }'
    register: curl_out
  - debug: var=curl_out.stdout_lines

  - name: Cleanup temp pod
    shell: "kubectl -n={{ namespace }} delete pod temp-jump"

  - name: Create load generator
    shell: |
      cat <<EOF | kubectl apply -f -
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: flog-producer
        namespace: "{{ namespace }}"
        labels:
          app: flog-producer
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: flog-producer
        template:
          metadata:
            labels:
              app: flog-producer
          spec:
            containers:
            - name: producer
              image: "joshuarobinson/flogkafka"
              command: ["bash"]
              args: ["-c", "/opt/flog -l -f json | /usr/local/bin/kafkacat -b confluentkafka:9092 -t flogs -P -z snappy -X acks=1"]
              imagePullPolicy: Always
              resources:
                requests:
                  cpu: "1000m"
            restartPolicy: Always

  - name: Update undo script
    lineinfile:
      path: undo-cluster.sh
      line: "kubectl -n={{ namespace }} delete deployment flog-producer"

  - name: Update undo script to delete buckets
    lineinfile:
      path: undo-cluster.sh
      line: "docker run -it --net=host --rm -e PUREFB_URL={{ FB_MGMTVIP }} -e PUREFB_API={{ FB_TOKEN }} joshuarobinson/s3manage delete bucket {{ item }}"
    loop:
      - "{{ kafka_bucket }}"
      - "es-snaprepo-{{ elastic_name }}"
      - "warehouse"
  
  - name: Add wait for any backgrounded tasks
    lineinfile:
      path: undo-cluster.sh
      line: wait
  - name: Add self-deletion to undo script
    lineinfile:
      path: undo-cluster.sh
      line: rm -- "$0"
  - name: Make undo script executable only by owner
    file: dest=undo-cluster.sh mode=700

  - debug:
      msg: "Elastic user password: {{ es_password }}"
