apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: {{ .Release.Name }}
spec:
  version: "{{ .Values.elasticsearch.version }}"
  nodeSets:
  - name: "hot-nodes-{{ .Values.elasticsearch.storageclass }}"
    count: {{ .Values.elasticsearch.nodecount }}
    podTemplate:
      metadata:
        labels:
          stack-monitoring.elastic.co/type: es
        annotations:
          co.elastic.logs/enabled: "true"
      spec:
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 32Gi
              cpu: 16
        initContainers:
        - name: install-plugins
          command:
          - sh
          - -c
          - |
            bin/elasticsearch-plugin remove repository-s3
            bin/elasticsearch-plugin install --batch repository-s3
        - name: add-access-keys
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: {{ .Release.Name }}-elastic-s3-keys
                key: access-key
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: {{ .Release.Name }}-elastic-s3-keys
                key: secret-key
          command:
            - sh
            - -c
            - |
              env | grep AWS_ACCESS_KEY_ID | awk -F= '{print $2}' | bin/elasticsearch-keystore add --stdin --force s3.client.default.access_key
              env | grep AWS_SECRET_ACCESS_KEY | awk -F= '{print $2}' | bin/elasticsearch-keystore add --stdin --force s3.client.default.secret_key
    config:
      node.store.allow_mmap: false
      node.roles: ["master", "data_hot", "ingest", "ml", "data_content"]
      thread_pool.snapshot.max: 8
      thread_pool.force_merge.size: 8
      indices.recovery.max_bytes_per_sec: "1gb"
      xpack.monitoring.elasticsearch.collection.enabled: false
      xpack.monitoring.collection.enabled: true
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 800Gi
        storageClassName: {{ .Values.elasticsearch.storageclass }}
  - name: "frozen-nodes"
    count: {{ .Values.elasticsearch.nodecount }}
    podTemplate:
      metadata:
        labels:
          stack-monitoring.elastic.co/type: es
        annotations:
          co.elastic.logs/enabled: "true"
      spec:
        volumes:
        - name: elasticsearch-data
          emptyDir: {}
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 8Gi
              cpu: 4
        initContainers:
        - name: install-plugins
          command:
          - sh
          - -c
          - |
            bin/elasticsearch-plugin remove repository-s3
            bin/elasticsearch-plugin install --batch repository-s3
        - name: add-access-keys
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: {{ .Release.Name }}-elastic-s3-keys
                key: access-key
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: {{ .Release.Name }}-elastic-s3-keys
                key: secret-key
          command:
            - sh
            - -c
            - |
              env | grep AWS_ACCESS_KEY_ID | awk -F= '{print $2}' | bin/elasticsearch-keystore add --stdin --force s3.client.default.access_key
              env | grep AWS_SECRET_ACCESS_KEY | awk -F= '{print $2}' | bin/elasticsearch-keystore add --stdin --force s3.client.default.secret_key
    config:
      node.store.allow_mmap: false
      node.roles: ["data_frozen"]
      thread_pool.snapshot.max: 8
      indices.recovery.max_bytes_per_sec: "1gb"
      xpack.searchable.snapshot.shared_cache.size: "1gb"
      xpack.monitoring.elasticsearch.collection.enabled: false
      xpack.monitoring.collection.enabled: true
